# Ollama Python Chat

Этот проект демонстрирует использование модели `Llama` от Ollama в Python. Включает инструменты для запуска модели, выполнения запросов и генерации текста.

## Описание проекта

Проект включает в себя класс `ModelRunner`, который предоставляет два основных метода для работы с моделью Llama:
- `query`: Выполняет запрос к модели в режиме чата.
- `generate`: Генерирует текст на основе переданного запроса.

Модель `Llama` — это мощная языковая модель, разработанная для обработки естественного языка. Она может использоваться для создания чат-ботов, генерации текста, перевода и других задач, связанных с обработкой текста.

### Основные функции:
- Запуск модели Llama.
- Выполнение запросов в режиме чата.
- Генерация текста на основе заданных промптов.

## Установка Ollama

Для работы с моделью `Llama`, необходимо установить Ollama. Инструкция по установке доступна на [официальном сайте Ollama](https://www.ollama.com):

1. Перейдите на страницу [скачивания Ollama](https://www.ollama.com/download) и следуйте инструкциям для вашей операционной системы.
   
2. После установки Ollama, вы можете загружать модели командой:

   ```bash
   ollama pull llama3.1
   ```

   Это команда по умолчанию установит модель в стандартную директорию Ollama.

### Рекомендации по оборудованию
Для корректной работы модели рекомендуется использовать видеокарту с объёмом памяти не менее 8 ГБ. Модели могут потребовать значительные ресурсы для обработки сложных запросов.

## Изменение директории для установки моделей

### Вариант 1: Изменение через команду Ollama
Вы можете указать директорию для сохранения моделей прямо в команде установки, используя опцию `--directory`:

```bash
ollama pull llama3.1 --directory /path/to/your/model/directory
```

### Вариант 2: Изменение через файл конфигурации
Также можно изменить путь для хранения моделей в конфигурационном файле Ollama:

1. Откройте файл конфигурации Ollama, который находится в домашней директории:
   
   Для macOS/Linux:
   ```bash
   ~/.ollama/config.yaml
   ```

   Для Windows:
   ```bash
   C:\Users\USERNAME\.ollama\config.yaml
   ```

2. Найдите или добавьте строку с указанием пути к директории для моделей:
   ```yaml
   models_dir: /path/to/your/model/directory
   ```

3. Сохраните файл. Теперь все новые модели будут загружаться в указанную директорию.

## Установка проекта

1. Клонируйте репозиторий на локальный компьютер:
   ```bash
   git clone https://github.com/AntonSHBK/ollama_python_chat
   cd ollama_python_chat
   ```

2. Создайте виртуальное окружение и активируйте его:
   ```bash
   python -m venv .venv
   # Для Windows
   .\.venv\Scripts\activate
   # Для Linux/MacOS
   source .venv/bin/activate
   ```

3. Установите зависимости:
   ```bash
   pip install -r requirements.txt
   ```

4. Убедитесь, что у вас установлен `ollama` и модель Llama, которую вы хотите использовать. Если модель еще не установлена, запустите:
   ```bash
   python install_model.py
   ```

## Настройка

Перед запуском проекта создайте файл `.env` в корне проекта и укажите имя модели:

```env
MODEL_NAME=llama3.1
```

## Запуск и тестирование

Для запуска проекта и тестирования функционала используйте скрипт `run.py`:

```bash
python run.py
```

### Пример использования

Скрипт `run.py` демонстрирует два основных метода:
1. **`query`**: Выполняет запрос к модели в режиме чата.
2. **`generate`**: Генерирует текст на основе переданного запроса.

В результате выполнения скрипта вы получите ответы модели на оба запроса, которые будут выведены на экран.

## О модели Llama

Llama — это языковая модель, разработанная для выполнения различных задач обработки естественного языка. Она может использоваться для создания диалоговых систем, генерации текстов, перевода и других задач, связанных с текстом. Модель отличается высокой точностью и способностью понимать контекст, что делает её эффективным инструментом для обработки сложных запросов.

### Официальные источники

- [Официальный сайт Ollama](https://www.ollama.com)
- [Документация по Llama](https://www.ollama.com/llama)

## Лицензия

Этот проект распространяется под лицензией MIT. Подробности смотрите в файле `LICENSE`.